\chapter{Background}
\label{chap:background}

\section{Leaky Integrate-and-Fire Neuron Model}
\label{sec:lif}

    \subsection{Dynamics of the Membrane Potential}
    \label{subsec:lif_dynamics}
        A leaky integrate-and-fire (LIF) neuron is a simple model of a neuron that captures the essential dynamics of a neuron. The LIF model is described by the differential equation:
        \begin{equation}
            \tau \frac{\mathrm{d}U(t)}{\mathrm{d}t} = -U(t) + I_{\text{in}}(t)
        \end{equation}
        where $U(t)$ is the membrane potential of the neuron, $\tau$ is the time constant of the neuron, and $I_{\text{in}}(t)$ is the input current to the neuron. The neuron fires a spike when the membrane potential reaches a threshold $U_{\text{th}}$. The membrane potential is then reset to a reset potential $U_{\text{reset}}$. 

        The equation above has an approximate solution given by:
        \begin{equation}
            U[t] = \beta U[t-1] + (1 - \beta) I_{\text{in}}[t]
        \end{equation}
        where $\beta = e^{-\Delta t/\tau}$, $\Delta t$ is the time step, and $I_{\text{in}}[t]$ is the input current at time $t$, defined by the following equation:
        \begin{equation}
            I_{\text{in}}[t] = W\cdot X[t]
        \end{equation}
        where $X[t]$ is the input spike train at time $t$, and $W$ is the weight matrix. 

        Since the weights $W$ are learnable parameters, one often merges the weights with the coefficient $(1 - \beta)$. 
        In the end, one obtains the following equation:
        \begin{equation}
            U[t] = \beta U[t-1] + W\cdot X[t] - S_{\text{out}}[t]\cdot\theta
        \end{equation}
        where $S_{\text{out}}[t]$ is the output spike train at time $t$, and $\theta$ is the threshold of the neuron. 
        By subtracting $S_{\text{out}}[t]\cdot\theta$ from the equation, one resets the membrane potential (soft reset) when the neuron fires a spike.

    \subsection{Spiking Mechanism}
    \label{subsec:lif_spiking}
        Usually the firing model of a neuron is very simple: 
        \begin{equation}
            S_{\text{out}}[t] = \begin{cases}
                1 & \text{if } U[t] \geq \theta \\
                0 & \text{otherwise}
            \end{cases}
        \end{equation}
        This is a heaviside step function, which is not differentiable. \\
        For the simplicity of analysis, one often considers $x := U[t] - \theta$ and $y := S_{\text{out}}[t]$.
        \begin{figure}[!htpb]
            \centering
            \begin{tikzpicture}
                \draw[->] (-5,0) -- (5,0) node[right] {x};
                \draw[->] (0,-1) -- (0,3) node[above] {y};
                \draw[color=teal] (-5,0) -- (0,0) -- (0,1) -- (5,1) node[above] {Heaviside Step Function};
                \draw[color=blue] plot[domain=-4:4, samples=100] (\x, {1/(1 + exp(-2*\x))});
                \matrix [draw=none, below left] at (current bounding box.north east) {
                    \node [blue]{Sigmoid $\alpha=2$}; \\
                };
            \end{tikzpicture}
            \caption{Comparison of the Heaviside Step Function and the Sigmoid Function}
            \label{fig:heaviside_sigmoid}
        \end{figure}
        At each time step, the membrane potential $U[t]$ is evaluated through the Heaviside step function, and
        the output is a binary value of 0 or 1, illustrated in Figure \ref{fig:heaviside_sigmoid}. The array of these binary values in the time domain is called the
        spike train of the neuron. \\
        After the neuron fires a spike, the membrane potential is reset to the reset potential. There are two ways
        to reset the membrane potential: hard reset and soft reset. In the hard reset, the membrane potential is
        immediately reset to the reset potential. In the soft reset, the membrane potential is subtracted by the
        threshold when the neuron fires a spike. Although the hard reset is more biologically plausible and more 
        efficient in terms of computation, the soft reset is more popular in practice because it often delivers 
        better performance in training.

\section{Training of Spiking Neural Networks}
\label{sec:snn_training}

    \subsection{Constructing Spiking Neural Networks}
    \label{subsec:snn_construct}
        There is in principle no ground-level limitation to the construction of spiking neural networks than the construction of traditional artificial neural networks. One of the most popular methods to construct a spiking neural network is to use the leaky integrate-and-fire (LIF) neuron node to replace the ReLU activation function in the ANNs. \\
        There are also other techniques to replace some components of the ANNs with certain tweaks for the SNNs (e.g. variants of batch normalization and attention mechanisms), but we will not discuss them in this thesis.

    \subsection{Backpropagation through Time}
    \label{subsec:snn_bptt}
        Although our brains are likely not trained by backpropagation, the gradient-based optimization is still the most popular and reliable method to train the neural networks. 

        Backpropagation through time (BPTT) is a method used to train recurrent neural networks (RNNs) by unfolding the network in time and applying backpropagation. The same method can be applied to train SNNs, as they are very similar to RNNs. 

        There are also other methods to train SNNs, like SLAYER \cite{Shrestha2018} and EXODUS \cite{bauer2022exodus} which utilize the vectorized model of the SNNs. However, we will focus on the temporal model of the SNNs in this thesis.

    \subsection{Surrogate Gradient}
    \label{subsec:lif_surrogate}
        Gradient-based optimization requires the activation function to be differentiable. However, the heaviside step function is not. Therefore, one often uses another differentiable function to approximate the heaviside step function in the backpropagation algorithm, e.g. the sigmoid function (see Figure \ref{fig:heaviside_sigmoid}): 
        \begin{equation}
            S_{\text{out}}'[t] = \frac{1}{1 + e^{-\alpha \cdot x}}
        \end{equation}
        It turns out that SNNs can tolerate such approximations, and the performance is quite reasonable. 
