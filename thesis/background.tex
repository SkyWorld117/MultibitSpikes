\chapter{Background}
\label{chap:background}

\section{Leaky Integrate-and-Fire Neuron Model}
\label{sec:lif}

    \subsection{Dynamics of the Membrane Potential}
    \label{subsec:lif_dynamics}
        A leaky integrate-and-fire (LIF) neuron is a simple model of that captures the essential dynamics of a neuron. The LIF model is described by the differential equation:
        \begin{equation}
            \tau \frac{\mathrm{d}U(t)}{\mathrm{d}t} = -U(t) + I_{\text{in}}(t)
        \end{equation}
        where $U(t)$ is the neuron's membrane potential, $\tau$ is its time constant, and $I_{\text{in}}(t)$ is the input current to the neuron. The neuron fires a spike when the membrane potential reaches a threshold $U_{\text{th}}$. The membrane potential is then reset to a resting potential $U_{\text{rest}}$. 

        The equation above has an approximate solution given by:
        \begin{equation}
            U[t] = \beta U[t-1] + (1 - \beta) I_{\text{in}}[t]
        \end{equation}
        where $\beta = e^{-\Delta t/\tau}$, $\Delta t$ is the time step, and $I_{\text{in}}[t]$ is the input current at time $t$, defined by the following equation:
        \begin{equation}
            I_{\text{in}}[t] = W\cdot X[t]
        \end{equation}
        where $X[t]$ is the input spike train at time $t$, and $W$ is the weight matrix. 

        Since the weights $W$ are learnable parameters, one often merges the weights with the coefficient $(1 - \beta)$, thus obtaining the following equation:
        \begin{equation}
            U[t] = \beta U[t-1] + W\cdot X[t] - S_{\text{out}}[t]\cdot\theta
        \end{equation}
        where $S_{\text{out}}[t]$ is the output spike train at time $t$, and $\theta$ is the threshold of the neuron. 
        By subtracting $S_{\text{out}}[t]\cdot\theta$ from the equation, one resets the membrane potential (soft reset) when the neuron fires a spike.

    \subsection{Spiking Mechanism}
    \label{subsec:lif_spiking}
        The firing model, as defined by Lapique in 1907, is modeled by the following equation:
        \begin{equation}
            S_{\text{out}}[t] = \begin{cases}
                1 & \text{if } U[t] \geq \theta \\
                0 & \text{otherwise}
            \end{cases}
        \end{equation}
        This is a heaviside step function, which is not differentiable. 

        For the simplicity of analysis, one often considers $x := U[t] - \theta$ and $y := S_{\text{out}}[t]$.
        \begin{figure}[!htpb]
            \centering
            \begin{tikzpicture}
                \draw[->] (-5,0) -- (5,0) node[right] {x};
                \draw[->] (0,-1) -- (0,3) node[above] {y};
                \draw[color=magenta] (-5,0) -- (0,0) -- (0,1) -- (5,1);
                \draw[color=blue] plot[domain=-4:4, samples=100] (\x, {1/(1 + exp(-2*\x))});
                \matrix [draw=none, below left] at (current bounding box.north east) {
                    \node [magenta]{Heaviside Step Function}; \\
                    \node [blue]{Sigmoid $\alpha=2$}; \\
                };
            \end{tikzpicture}
            \caption{Comparison of the Heaviside Step Function and the Sigmoid Function}
            \label{fig:heaviside_sigmoid}
        \end{figure}
        At each time step, the membrane potential $U[t]$ is evaluated through the Heaviside step function, and the output is a binary value of 0 or 1, as illustrated in Figure \ref{fig:heaviside_sigmoid}. The array of these binary values in the time domain is called the spike train of the neuron. 

        After the neuron fires a spike, the membrane potential is reset to the resting potential. There are two ways to reset the membrane potential: hard reset and soft reset. In the hard reset, the membrane potential is immediately set to the resting potential. In the soft reset, the membrane potential is subtracted by the threshold when the neuron fires a spike. Although the hard reset is more biologically plausible and more efficient in terms of computation, the soft reset is more popular in practice because it often delivers better performance in training \cite{10242251}. In this thesis we focus on the soft reset.

\section{Training of Spiking Neural Networks}
\label{sec:snn_training}

    \subsection{Constructing Spiking Neural Networks}
    \label{subsec:snn_construct}
        The construction of spiking neural networks is similar with the construction of traditional artificial neural networks. One of the most popular methods to construct a spiking neural network is to use the leaky integrate-and-fire (LIF) neuron node to replace the ReLU activation function in the ANNs. 

        There are also other techniques to replace some components of the ANNs with certain tweaks for the SNNs, e.g. variants of batch normalization (such as Spike-Norm \cite{10.3389/fnins.2019.00095}) and attention mechanisms (such as Spiking RWKV \cite{zhu2024spikegptgenerativepretrainedlanguage}), but they are outside of the scope of this thesis. 

    \subsection{Backpropagation through Time}
    \label{subsec:snn_bptt}
        Although our brains are likely not trained by backpropagation, the gradient-based optimization is still the most popular and reliable method to train the neural networks. 

        Backpropagation through time (BPTT) is a method used to train recurrent neural networks (RNNs) \cite{6302929} by unfolding the network in time and applying backpropagation. The same method can be applied to train SNNs, as they are very similar to RNNs. 

        There are also other methods to train SNNs, like SLAYER \cite{Shrestha2018} and EXODUS \cite{bauer2022exodus} which utilize the vectorized model of the SNNs. However, we will focus on the temporal model of the SNNs in this thesis.

    \subsection{Surrogate Gradient}
    \label{subsec:lif_surrogate}
        Gradient-based optimization requires the activation function to be differentiable, which the Heaviside step function is not. Therefore, one often uses another differentiable function to approximate the Heaviside step function in the backpropagation algorithm \cite{8891809}, e.g. the sigmoid function (see Figure \ref{fig:heaviside_sigmoid}): 
        \begin{equation}
            S_{\text{out}}'[t] = \frac{1}{1 + e^{-\alpha \cdot x}}
        \end{equation}
        It turns out that SNNs can tolerate such approximations, and it has become the state-of-the-art method to train performant SNNs.
