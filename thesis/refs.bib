@article{lapicque1907louis,
  title={Recherches quantitatives sur l'excitation electrique des nerfs traitee comme une polarization.},
  author={Lapique, Louis},
  journal={J. of Physiol. and Pathology},
  volume={9},
  pages={620--635},
  year={1907}
}

@InCollection{Shrestha2018,
  author    = {Shrestha, Sumit Bam and Orchard, Garrick},
  title     = {{SLAYER}: Spike Layer Error Reassignment in Time},
  booktitle = {Advances in Neural Information Processing Systems 31},
  publisher = {Curran Associates, Inc.},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {1419--1428},
  url       = {http://papers.nips.cc/paper/7415-slayer-spike-layer-error-reassignment-in-time.pdf},
}

@article{bauer2022exodus,
  title={EXODUS: Stable and Efficient Training of Spiking Neural Networks},
  author={Bauer, Felix Christian and Lenz, Gregor and Haghighatshoar, Saeid and Sheik, Sadique},
  journal={arXiv preprint arXiv:2205.10242},
  year={2022}
}

@article{doi:10.1126/sciadv.adi1480,
  author = {Wei Fang  and Yanqi Chen  and Jianhao Ding  and Zhaofei Yu  and Timothée Masquelier  and Ding Chen  and Liwei Huang  and Huihui Zhou  and Guoqi Li  and Yonghong Tian },
  title = {SpikingJelly: An open-source machine learning infrastructure platform for spike-based intelligence},
  journal = {Science Advances},
  volume = {9},
  number = {40},
  pages = {eadi1480},
  year = {2023},
  doi = {10.1126/sciadv.adi1480},
  URL = {https://www.science.org/doi/abs/10.1126/sciadv.adi1480},
  eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.adi1480},
  abstract = {Spiking neural networks (SNNs) aim to realize brain-inspired intelligence on neuromorphic chips with high energy efficiency by introducing neural dynamics and spike properties. As the emerging spiking deep learning paradigm attracts increasing interest, traditional programming frameworks cannot meet the demands of automatic differentiation, parallel computation acceleration, and high integration of processing neuromorphic datasets and deployment. In this work, we present the SpikingJelly framework to address the aforementioned dilemma. We contribute a full-stack toolkit for preprocessing neuromorphic datasets, building deep SNNs, optimizing their parameters, and deploying SNNs on neuromorphic chips. Compared to existing methods, the training of deep SNNs can be accelerated 11×, and the superior extensibility and flexibility of SpikingJelly enable users to accelerate custom models at low costs through multilevel inheritance and semiautomatic code generation. SpikingJelly paves the way for synthesizing truly energy-efficient SNN-based machine intelligence systems, which will enrich the ecology of neuromorphic computing. Motivation and introduction of the software framework SpikingJelly for spiking deep learning.}
}

@online{xiao2017/online,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@techreport{Krizhevsky2009,
  author = {Alex Krizhevsky},
  title = {Learning Multiple Layers of Features from Tiny Images},
  year = {2009}
}

@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{10.3389/fnins.2015.00437,
  AUTHOR={Orchard, Garrick  and Jayawant, Ajinkya  and Cohen, Gregory K.  and Thakor, Nitish },
  TITLE={Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades},
  JOURNAL={Frontiers in Neuroscience},
  VOLUME={9},
  YEAR={2015},
  URL={https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2015.00437},
  DOI={10.3389/fnins.2015.00437},
  ISSN={1662-453X},
  ABSTRACT={<p>Creating datasets for Neuromorphic Vision is a challenging task. A lack of available recordings from Neuromorphic Vision sensors means that data must typically be recorded specifically for dataset creation rather than collecting and labeling existing data. The task is further complicated by a desire to simultaneously provide traditional frame-based recordings to allow for direct comparison with traditional Computer Vision algorithms. Here we propose a method for converting existing Computer Vision static image datasets into Neuromorphic Vision datasets using an actuated pan-tilt camera platform. Moving the sensor rather than the scene or image is a more biologically realistic approach to sensing and eliminates timing artifacts introduced by monitor updates when simulating motion on a computer monitor. We present conversion of two popular image datasets (MNIST and Caltech101) which have played important roles in the development of Computer Vision, and we provide performance metrics on these datasets using spike-based recognition algorithms. This work contributes datasets for future use in the field, as well as results from spike-based algorithms against which future works can compare. Furthermore, by converting datasets already popular in Computer Vision, we enable more direct comparison with frame-based approaches.</p>}
}

@INPROCEEDINGS{8100264,
  author={Amir, Arnon and Taba, Brian and Berg, David and Melano, Timothy and McKinstry, Jeffrey and Di Nolfo, Carmelo and Nayak, Tapan and Andreopoulos, Alexander and Garreau, Guillaume and Mendoza, Marcela and Kusnitz, Jeff and Debole, Michael and Esser, Steve and Delbruck, Tobi and Flickner, Myron and Modha, Dharmendra},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={A Low Power, Fully Event-Based Gesture Recognition System}, 
  year={2017},
  volume={},
  number={},
  pages={7388-7397},
  keywords={Cameras;Neurons;Gesture recognition;Voltage control;Real-time systems;Sensors;Feature extraction},
  doi={10.1109/CVPR.2017.781}
}

@ARTICLE{8259423,
  author={Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
  journal={IEEE Micro}, 
  title={Loihi: A Neuromorphic Manycore Processor with On-Chip Learning}, 
  year={2018},
  volume={38},
  number={1},
  pages={82-99},
  keywords={Neurons;Computer architecture;Computational modeling;Neuromorphics;Biological neural networks;Algorithm design and analysis;neuromorphic computing;machine learning;artificial intelligence},
  doi={10.1109/MM.2018.112130359}}

@ARTICLE{7229264,
  author={Akopyan, Filipp and Sawada, Jun and Cassidy, Andrew and Alvarez-Icaza, Rodrigo and Arthur, John and Merolla, Paul and Imam, Nabil and Nakamura, Yutaka and Datta, Pallab and Nam, Gi-Joon and Taba, Brian and Beakes, Michael and Brezzo, Bernard and Kuang, Jente B. and Manohar, Rajit and Risk, William P. and Jackson, Bryan and Modha, Dharmendra S.},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
  title={TrueNorth: Design and Tool Flow of a 65 mW 1 Million Neuron Programmable Neurosynaptic Chip}, 
  year={2015},
  volume={34},
  number={10},
  pages={1537-1557},
  keywords={Computer architecture;Synchronization;Nerve fibers;Real-time systems;Architecture;Biological neural networks;Asynchronous circuits;asynchronous communication;design automation;design methodology;image recognition;logic design;low-power electronics;neural network hardware;neural networks;neuromorphics;parallel architectures;real-time systems;synchronous circuits;very large-scale integration},
  doi={10.1109/TCAD.2015.2474396}}

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
} 

@article{oh2004gpu,
author = {Oh, Kyoungsu and Jung, Keechul},
year = {2004},
month = {06},
pages = {1311-1314},
title = {GPU implementation of neural networks},
volume = {37},
journal = {Pattern Recognition},
doi = {10.1016/j.patcog.2004.01.013}
}

@article{doi:10.1073/pnas.2107022118,
author = {Vijay Balasubramanian },
title = {Brain power},
journal = {Proceedings of the National Academy of Sciences},
volume = {118},
number = {32},
pages = {e2107022118},
year = {2021},
doi = {10.1073/pnas.2107022118},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2107022118},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2107022118}}

@article{DEVRIES20232191,
title = {The growing energy footprint of artificial intelligence},
journal = {Joule},
volume = {7},
number = {10},
pages = {2191-2194},
year = {2023},
issn = {2542-4351},
doi = {https://doi.org/10.1016/j.joule.2023.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2542435123003653},
author = {Alex {de Vries}},
abstract = {Alex de Vries is a PhD candidate at the VU Amsterdam School of Business and Economics and the founder of Digiconomist, a research company dedicated to exposing the unintended consequences of digital trends. His research focuses on the environmental impact of emerging technologies and has played a major role in the global discussion regarding the sustainability of blockchain technology.}
}

@article{jphysiol.1952.sp004764,
author = {Hodgkin, A. L. and Huxley, A. F.},
title = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
journal = {The Journal of Physiology},
volume = {117},
number = {4},
pages = {500-544},
doi = {https://doi.org/10.1113/jphysiol.1952.sp004764},
url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1952.sp004764},
eprint = {https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1952.sp004764},
year = {1952}
}

@ARTICLE{10242251,
  author={Eshraghian, Jason K. and Ward, Max and Neftci, Emre O. and Wang, Xinxin and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and Jeong, Doo Seok and Lu, Wei D.},
  journal={Proceedings of the IEEE}, 
  title={Training Spiking Neural Networks Using Lessons From Deep Learning}, 
  year={2023},
  volume={111},
  number={9},
  pages={1016-1054},
  keywords={Deep learning;Neuromorphics;Neurons;Biological neural networks;Training;Brain modeling;Australia;Electronic learning;Brain modeling;Tutorials;Deep learning;neural code;neuromorphic;online learning;spiking neural networks (SNNs)},
  doi={10.1109/JPROC.2023.3308088}
}

@ARTICLE{10.3389/fnins.2019.00095,
AUTHOR={Sengupta, Abhronil  and Ye, Yuting  and Wang, Robert  and Liu, Chiao  and Roy, Kaushik },
TITLE={Going Deeper in Spiking Neural Networks: VGG and Residual Architectures},
JOURNAL={Frontiers in Neuroscience},
VOLUME={13},
YEAR={2019},
URL={https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2019.00095},
DOI={10.3389/fnins.2019.00095},
ISSN={1662-453X},
ABSTRACT={<p>Over the past few years, Spiking Neural Networks (SNNs) have become popular as a possible pathway to enable low-power event-driven neuromorphic hardware. However, their application in machine learning have largely been limited to very shallow neural network architectures for simple problems. In this paper, we propose a novel algorithmic technique for generating an SNN with a deep architecture, and demonstrate its effectiveness on complex visual recognition problems such as CIFAR-10 and ImageNet. Our technique applies to both VGG and Residual network architectures, with significantly better accuracy than the state-of-the-art. Finally, we present analysis of the sparse event-driven computations to demonstrate reduced hardware overhead when operating in the spiking domain.</p>}
}

@misc{zhu2024spikegptgenerativepretrainedlanguage,
      title={SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks}, 
      author={Rui-Jie Zhu and Qihang Zhao and Guoqi Li and Jason K. Eshraghian},
      year={2024},
      eprint={2302.13939},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13939}, 
}

@ARTICLE{8891809,
  author={Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  journal={IEEE Signal Processing Magazine}, 
  title={Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-Based Optimization to Spiking Neural Networks}, 
  year={2019},
  volume={36},
  number={6},
  pages={51-63},
  keywords={Neural networks;Fault tolerance;Energy efficiency;Biological system modeling},
  doi={10.1109/MSP.2019.2931595}
}

@article{jphysiol.1962.sp006837,
author = {Hubel, D. H. and Wiesel, T. N.},
title = {Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
journal = {The Journal of Physiology},
volume = {160},
number = {1},
pages = {106-154},
doi = {https://doi.org/10.1113/jphysiol.1962.sp006837},
url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1962.sp006837},
eprint = {https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1962.sp006837},
year = {1962}
}

@article{
doi:10.1073/pnas.0610368104,
author = {Steven M. Chase  and Eric D. Young },
title = {First-spike latency information in single neurons increases when referenced to population onset},
journal = {Proceedings of the National Academy of Sciences},
volume = {104},
number = {12},
pages = {5175-5180},
year = {2007},
doi = {10.1073/pnas.0610368104},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0610368104},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0610368104},
abstract = {It is well known that many stimulus parameters, such as sound location in the auditory system or contrast in the visual system, can modulate the timing of the first spike in sensory neurons. Could first-spike latency be a candidate neural code? Most studies measuring first-spike latency information assume that the brain has an independent reference for stimulus onset from which to extract latency. This assumption creates an obvious confound that casts doubt on the feasibility of first-spike latency codes. If latency is measured relative to an internal reference of stimulus onset calculated from the responses of the neural population, the information conveyed by the latency of single neurons might decrease because of correlated changes in latency across the population. Here we assess the effects of a realistic model of stimulus onset detection on the first-spike latency information conveyed by single neurons in the auditory system. Contrary to expectation, we find that on average, the information contained in single neurons does not decrease; in fact, the majority of neurons show a slight increase in the information conveyed by latency referenced to a population onset. Our results show that first-spike latency codes are a feasible mechanism for information transfer even when biologically plausible estimates of stimulus onset are taken into account.}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
} 

@software{lenz_2021_5079802,
  author       = {Lenz, Gregor and
                  Chaney, Kenneth and
                  Shrestha, Sumit Bam and
                  Oubari, Omar and
                  Picaud, Serge and
                  Zarrella, Guido},
  title        = {Tonic: event-based datasets and transformations.},
  month        = jul,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {0.4.0},
  doi          = {10.5281/zenodo.5079802},
  url          = {https://doi.org/10.5281/zenodo.5079802},
}

@inproceedings{10.1145/1873951.1874254,
author = {Marcel, S\'{e}bastien and Rodriguez, Yann},
title = {Torchvision the machine-vision package of torch},
year = {2010},
isbn = {9781605589336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1873951.1874254},
doi = {10.1145/1873951.1874254},
abstract = {This paper presents Torchvision an open source machine vision package for Torch. Torch is a machine learning library providing a series of the state-of-the-art algorithms such as Neural Networks, Support Vector Machines, Gaussian Mixture Models, Hidden Markov Models and many others. Torchvision provides additional functionalities to manipulate and process images with standard image processing algorithms. Hence, the resulting images can be used directly with the Torch machine learning algorithms as Torchvision is fully integrated with Torch. Both Torch and Torchvision are written in C++ language and are publicly available under the Free-BSD License.},
booktitle = {Proceedings of the 18th ACM International Conference on Multimedia},
pages = {1485–1488},
numpages = {4},
keywords = {face detection and recognition, machine learning, open source, pattern recognition, vision},
location = {Firenze, Italy},
series = {MM '10}
}

@INPROCEEDINGS{9534087,
  author={Putra, Rachmad Vidya Wicaksana and Shafique, Muhammad},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Q-SpiNN: A Framework for Quantizing Spiking Neural Networks}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  keywords={Quantization (signal);Neurons;Employment;Membrane potentials;Biological neural networks},
  doi={10.1109/IJCNN52387.2021.9534087}
}

@inproceedings{10.1145/3664647.3681186,
author = {Wei, Wenjie and Liang, Yu and Belatreche, Ammar and Xiao, Yichen and Cao, Honglin and Ren, Zhenbang and Wang, Guoqing and Zhang, Malu and Yang, Yang},
title = {Q-SNNs: Quantized Spiking Neural Networks},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681186},
doi = {10.1145/3664647.3681186},
abstract = {Brain-inspired Spiking Neural Networks (SNNs) leverage sparse spikes to represent information and process them in an asynchronous event-driven manner, offering an energy-efficient paradigm for the next generation of machine intelligence. However, the current focus within the SNN community prioritizes accuracy optimization through the development of large-scale models, limiting their viability in resource-constrained and low-power edge devices. To address this challenge, we introduce a lightweight and hardware-friendly Quantized SNN (Q-SNN) that applies quantization to both synaptic weights and membrane potentials. By significantly compressing these two key elements, the proposed Q-SNNs substantially reduce both memory usage and computational complexity. Moreover, to prevent the performance degradation caused by this compression, we present a new Weight-Spike Dual Regulation (WS-DR) method inspired by information entropy theory. Experimental evaluations on various datasets, including static and neuromorphic, demonstrate that our Q-SNNs outperform existing methods in terms of both model size and accuracy. These state-of-the-art results in efficiency and efficacy suggest that the proposed method can significantly improve edge intelligent computing.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {8441–8450},
numpages = {10},
keywords = {neuromorphic datasets, quantization, spiking neural networks},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

  


