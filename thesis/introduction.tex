\chapter{Introduction}
\label{chap:introduction}

Artificial neural networks (ANNs) \cite{mcculloch1943logical} have been widely used in machine learning and artificial intelligence. The computational principle behind ANNs is mostly matrix multiplications which can be efficiently implemented on modern hardware like GPUs \cite{oh2004gpu}. Although such operations can be excellently parallelized and accelerated, the energy consumption of ANNs is still high.

On the other hand, human brains are much more energy-efficient than ANNs. The energy consumption of the human brain is estimated to be
around 20 watts \cite{doi:10.1073/pnas.2107022118}, while the energy consumption of a large ANN model like ChatGPT is estimated to be around 23.5 megawatts \cite{DEVRIES20232191}. The human brain is estimated to be over one million times more energy-efficient than ChatGPT. One of the reasons for this energy efficiency is the difference between the neuron models used in ANNs and the biological neurons in the human brain. Unlike ANNs, which use floating-point numbers for numerical computation, biological neurons communicate with each other using spike trains and rely on temporal information \cite{jphysiol.1952.sp004764}.

To model such biological neurons and deploy them in artificial neural networks, spiking neural networks (SNNs) have been proposed. The most commonly used neuron model in SNNs is the leaky integrate-and-fire (LIF) \cite{lapicque1907louis}. In this model, a neuron's membrane potential is increased by incoming spikes and decreased by a leak term. When the membrane potential reaches a threshold, the neuron fires a spike and resets its membrane potential. The communication between neurons in SNNs is done by sending spikes, which are binary events.

In this thesis, we propose a novel neuron model where neurons can fire at multiple spiking levels. Instead of firing a single spike when the membrane potential reaches a threshold, neurons can fire multiple spikes at different levels. We call this model the multi-bit spike train model. The motivation behind this model is to increase the communication bandwidth between neurons and enable more efficient training or inference in SNNs. We explore the properties of this model and compare it with the traditional LIF neuron model.

We experiment on various datasets and tasks with the multi-bit spike train model and show that it can achieve better performance than the classic 1-bit spike train model in the most cases. Finally, we also present a energy consumption model of the multi-bit spike train model and show that it can be more energy-efficient than the LIF neuron model assuming certain hardware implementations.

%TODO: Add precise results here