@article{lapicque1907louis,
  title={Recherches quantitatives sur l'excitation electrique des nerfs traitee comme une polarization.},
  author={Lapique, Louis},
  journal={J. of Physiol. and Pathology},
  volume={9},
  pages={620--635},
  year={1907}
}

@InCollection{Shrestha2018,
  author    = {Shrestha, Sumit Bam and Orchard, Garrick},
  title     = {{SLAYER}: Spike Layer Error Reassignment in Time},
  booktitle = {Advances in Neural Information Processing Systems 31},
  publisher = {Curran Associates, Inc.},
  year      = {2018},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {1419--1428},
  url       = {http://papers.nips.cc/paper/7415-slayer-spike-layer-error-reassignment-in-time.pdf},
}

@article{bauer2022exodus,
  title={EXODUS: Stable and Efficient Training of Spiking Neural Networks},
  author={Bauer, Felix Christian and Lenz, Gregor and Haghighatshoar, Saeid and Sheik, Sadique},
  journal={arXiv preprint arXiv:2205.10242},
  year={2022}
}

@article{doi:10.1126/sciadv.adi1480,
  author = {Wei Fang  and Yanqi Chen  and Jianhao Ding  and Zhaofei Yu  and Timothée Masquelier  and Ding Chen  and Liwei Huang  and Huihui Zhou  and Guoqi Li  and Yonghong Tian },
  title = {SpikingJelly: An open-source machine learning infrastructure platform for spike-based intelligence},
  journal = {Science Advances},
  volume = {9},
  number = {40},
  pages = {eadi1480},
  year = {2023},
  doi = {10.1126/sciadv.adi1480},
  URL = {https://www.science.org/doi/abs/10.1126/sciadv.adi1480},
  eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.adi1480},
  abstract = {Spiking neural networks (SNNs) aim to realize brain-inspired intelligence on neuromorphic chips with high energy efficiency by introducing neural dynamics and spike properties. As the emerging spiking deep learning paradigm attracts increasing interest, traditional programming frameworks cannot meet the demands of automatic differentiation, parallel computation acceleration, and high integration of processing neuromorphic datasets and deployment. In this work, we present the SpikingJelly framework to address the aforementioned dilemma. We contribute a full-stack toolkit for preprocessing neuromorphic datasets, building deep SNNs, optimizing their parameters, and deploying SNNs on neuromorphic chips. Compared to existing methods, the training of deep SNNs can be accelerated 11×, and the superior extensibility and flexibility of SpikingJelly enable users to accelerate custom models at low costs through multilevel inheritance and semiautomatic code generation. SpikingJelly paves the way for synthesizing truly energy-efficient SNN-based machine intelligence systems, which will enrich the ecology of neuromorphic computing. Motivation and introduction of the software framework SpikingJelly for spiking deep learning.}
}

@online{xiao2017/online,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@techreport{Krizhevsky2009,
  author = {Alex Krizhevsky},
  title = {Learning Multiple Layers of Features from Tiny Images},
  year = {2009}
}

@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{10.3389/fnins.2015.00437,
  AUTHOR={Orchard, Garrick  and Jayawant, Ajinkya  and Cohen, Gregory K.  and Thakor, Nitish },
  TITLE={Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades},
  JOURNAL={Frontiers in Neuroscience},
  VOLUME={9},
  YEAR={2015},
  URL={https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2015.00437},
  DOI={10.3389/fnins.2015.00437},
  ISSN={1662-453X},
  ABSTRACT={<p>Creating datasets for Neuromorphic Vision is a challenging task. A lack of available recordings from Neuromorphic Vision sensors means that data must typically be recorded specifically for dataset creation rather than collecting and labeling existing data. The task is further complicated by a desire to simultaneously provide traditional frame-based recordings to allow for direct comparison with traditional Computer Vision algorithms. Here we propose a method for converting existing Computer Vision static image datasets into Neuromorphic Vision datasets using an actuated pan-tilt camera platform. Moving the sensor rather than the scene or image is a more biologically realistic approach to sensing and eliminates timing artifacts introduced by monitor updates when simulating motion on a computer monitor. We present conversion of two popular image datasets (MNIST and Caltech101) which have played important roles in the development of Computer Vision, and we provide performance metrics on these datasets using spike-based recognition algorithms. This work contributes datasets for future use in the field, as well as results from spike-based algorithms against which future works can compare. Furthermore, by converting datasets already popular in Computer Vision, we enable more direct comparison with frame-based approaches.</p>}
}

@INPROCEEDINGS{8100264,
  author={Amir, Arnon and Taba, Brian and Berg, David and Melano, Timothy and McKinstry, Jeffrey and Di Nolfo, Carmelo and Nayak, Tapan and Andreopoulos, Alexander and Garreau, Guillaume and Mendoza, Marcela and Kusnitz, Jeff and Debole, Michael and Esser, Steve and Delbruck, Tobi and Flickner, Myron and Modha, Dharmendra},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={A Low Power, Fully Event-Based Gesture Recognition System}, 
  year={2017},
  volume={},
  number={},
  pages={7388-7397},
  keywords={Cameras;Neurons;Gesture recognition;Voltage control;Real-time systems;Sensors;Feature extraction},
  doi={10.1109/CVPR.2017.781}
}

@ARTICLE{8259423,
  author={Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
  journal={IEEE Micro}, 
  title={Loihi: A Neuromorphic Manycore Processor with On-Chip Learning}, 
  year={2018},
  volume={38},
  number={1},
  pages={82-99},
  keywords={Neurons;Computer architecture;Computational modeling;Neuromorphics;Biological neural networks;Algorithm design and analysis;neuromorphic computing;machine learning;artificial intelligence},
  doi={10.1109/MM.2018.112130359}}

@ARTICLE{7229264,
  author={Akopyan, Filipp and Sawada, Jun and Cassidy, Andrew and Alvarez-Icaza, Rodrigo and Arthur, John and Merolla, Paul and Imam, Nabil and Nakamura, Yutaka and Datta, Pallab and Nam, Gi-Joon and Taba, Brian and Beakes, Michael and Brezzo, Bernard and Kuang, Jente B. and Manohar, Rajit and Risk, William P. and Jackson, Bryan and Modha, Dharmendra S.},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
  title={TrueNorth: Design and Tool Flow of a 65 mW 1 Million Neuron Programmable Neurosynaptic Chip}, 
  year={2015},
  volume={34},
  number={10},
  pages={1537-1557},
  keywords={Computer architecture;Synchronization;Nerve fibers;Real-time systems;Architecture;Biological neural networks;Asynchronous circuits;asynchronous communication;design automation;design methodology;image recognition;logic design;low-power electronics;neural network hardware;neural networks;neuromorphics;parallel architectures;real-time systems;synchronous circuits;very large-scale integration},
  doi={10.1109/TCAD.2015.2474396}}

@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
} 

@article{oh2004gpu,
author = {Oh, Kyoungsu and Jung, Keechul},
year = {2004},
month = {06},
pages = {1311-1314},
title = {GPU implementation of neural networks},
volume = {37},
journal = {Pattern Recognition},
doi = {10.1016/j.patcog.2004.01.013}
}

@article{doi:10.1073/pnas.2107022118,
author = {Vijay Balasubramanian },
title = {Brain power},
journal = {Proceedings of the National Academy of Sciences},
volume = {118},
number = {32},
pages = {e2107022118},
year = {2021},
doi = {10.1073/pnas.2107022118},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2107022118},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2107022118}}

@article{DEVRIES20232191,
title = {The growing energy footprint of artificial intelligence},
journal = {Joule},
volume = {7},
number = {10},
pages = {2191-2194},
year = {2023},
issn = {2542-4351},
doi = {https://doi.org/10.1016/j.joule.2023.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2542435123003653},
author = {Alex {de Vries}},
abstract = {Alex de Vries is a PhD candidate at the VU Amsterdam School of Business and Economics and the founder of Digiconomist, a research company dedicated to exposing the unintended consequences of digital trends. His research focuses on the environmental impact of emerging technologies and has played a major role in the global discussion regarding the sustainability of blockchain technology.}
}

@article{jphysiol.1952.sp004764,
author = {Hodgkin, A. L. and Huxley, A. F.},
title = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
journal = {The Journal of Physiology},
volume = {117},
number = {4},
pages = {500-544},
doi = {https://doi.org/10.1113/jphysiol.1952.sp004764},
url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1952.sp004764},
eprint = {https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1952.sp004764},
year = {1952}
}

@ARTICLE{10242251,
  author={Eshraghian, Jason K. and Ward, Max and Neftci, Emre O. and Wang, Xinxin and Lenz, Gregor and Dwivedi, Girish and Bennamoun, Mohammed and Jeong, Doo Seok and Lu, Wei D.},
  journal={Proceedings of the IEEE}, 
  title={Training Spiking Neural Networks Using Lessons From Deep Learning}, 
  year={2023},
  volume={111},
  number={9},
  pages={1016-1054},
  keywords={Deep learning;Neuromorphics;Neurons;Biological neural networks;Training;Brain modeling;Australia;Electronic learning;Brain modeling;Tutorials;Deep learning;neural code;neuromorphic;online learning;spiking neural networks (SNNs)},
  doi={10.1109/JPROC.2023.3308088}
}

@ARTICLE{10.3389/fnins.2019.00095,
AUTHOR={Sengupta, Abhronil  and Ye, Yuting  and Wang, Robert  and Liu, Chiao  and Roy, Kaushik },
TITLE={Going Deeper in Spiking Neural Networks: VGG and Residual Architectures},
JOURNAL={Frontiers in Neuroscience},
VOLUME={13},
YEAR={2019},
URL={https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2019.00095},
DOI={10.3389/fnins.2019.00095},
ISSN={1662-453X},
ABSTRACT={<p>Over the past few years, Spiking Neural Networks (SNNs) have become popular as a possible pathway to enable low-power event-driven neuromorphic hardware. However, their application in machine learning have largely been limited to very shallow neural network architectures for simple problems. In this paper, we propose a novel algorithmic technique for generating an SNN with a deep architecture, and demonstrate its effectiveness on complex visual recognition problems such as CIFAR-10 and ImageNet. Our technique applies to both VGG and Residual network architectures, with significantly better accuracy than the state-of-the-art. Finally, we present analysis of the sparse event-driven computations to demonstrate reduced hardware overhead when operating in the spiking domain.</p>}
}

@misc{zhu2024spikegptgenerativepretrainedlanguage,
      title={SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks}, 
      author={Rui-Jie Zhu and Qihang Zhao and Guoqi Li and Jason K. Eshraghian},
      year={2024},
      eprint={2302.13939},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13939}, 
}

@ARTICLE{8891809,
  author={Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  journal={IEEE Signal Processing Magazine}, 
  title={Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-Based Optimization to Spiking Neural Networks}, 
  year={2019},
  volume={36},
  number={6},
  pages={51-63},
  keywords={Neural networks;Fault tolerance;Energy efficiency;Biological system modeling},
  doi={10.1109/MSP.2019.2931595}
}
