\chapter{Related Work}
\label{chap:related}

We mainly used the LIF neuron model \cite{lapicque1907louis} in this thesis, which is a simple model that does not capture the full complexity of biological neurons. Hudgkin and Huxley \cite{jphysiol.1952.sp004764} proposed a more detailed model that includes the dynamics of the ion channels, which is more biologically plausible but also computationally more expensive. Most of the models, e.g. Izhikevich's model \cite{1257420}, focus on some specific dynamics of the biological neurons and enable some tradeoffs. 

Although BPTT is widely used in training deep large SNNs, other methods such as spike-timing-dependent plasticity (STDP) \cite{doi:10.1126/science.275.5297.213} are more biologically plausible with clear evidence in the brain. Methods like SLAYER \cite{Shrestha2018} and EXODUS \cite{bauer2022exodus} focus more on the efficient computation of the gradients in the SNNs instead of the biological plausibility.

Typical constructions of SNNs include replacing the ReLU activation function in ANNs with the LIF neuron node. Due to the non-differentiability of the spike function and the time dependency of the membrane potential, the training of SNNs is more challenging than ANNs. Many methods, e.g. Spike-Norm \cite{10.3389/fnins.2019.00095} and Spiking RWKV \cite{zhu2024spikegptgenerativepretrainedlanguage} focus on replacing some components of ANNs with certain tweaks for SNNs to maintain the performance witnessed in ANNs while easing the training process.

As the demand for AI applications grows, the energy efficiency of the hardware becomes more important. Neuromorphic hardwares like IBM's TrueNorth \cite{7229264} and Intel's Loihi \cite{8259423} are designed to exploit the fact that within a discrete time step, there is no need for synchronous communication between the neurons, as each spike increases the membrane potential of the target neuron and such operation is commutative. This allows a partially asynchronous hardware design that can be more energy efficient than traditional GPUs. 
However, they lack the capability of training the network on the hardware, which is still done on traditional GPUs.

Quantization is a technique to reduce the memory footprint and computation cost of the neural networks by reducing the precision of the parameters. While large language models implemented in ANNs can be quantized to 4-bit precision \cite{ashkboos2024quarotoutlierfree4bitinference}, SNNs can be quantized to even lower precision \cite{10.1145/3664647.3681186} which should be able to achieve higher energy efficiency.

There exists work that shows the potential of using multi-bit spike trains in SNNs. In \cite{xiao2024multibitmechanismnovelinformation} the authors propose to use two integer number of bits to describe the ratio of the membrane potential to the threshold. They were able to show that the multi-bit spike train model can achieve better accuracy. 
Our work differs from \cite{xiao2024multibitmechanismnovelinformation} in that we divide the range of the membrane potential into $2^n-1$ intervals and center the sigmoid function accordingly. This allows us to show the improvement in the convergence speed and the performance of the network.