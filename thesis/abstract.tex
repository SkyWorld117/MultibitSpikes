\begin{abstract}
  Spiking Neural Networks (SNNs) represent an emerging paradigm in machine learning, inspired by biological neurons, with potential for greater energy efficiency compared to traditional Artificial Neural Networks (ANNs). Unlike ANNs, which use continuous floating-point outputs, SNNs produce discrete (binary) spikes when a neuronâ€™s membrane voltage exceeds a threshold.

  In this thesis we propose a novel firing model where neurons fire at multiple spiking levels (multi-bit spike trains) when their membrane potentials reach corresponding thresholds. We show that this model enables faster convergence and better performance on various tasks compared to the traditional 1-bit spike train model, while maintaining the energy efficiency of SNNs. We also present an energy consumption model in a real-world workflow and show how the multi-bit spike train model can be beneficial in terms of energy efficiency.
\end{abstract}

%TODO: Add exact results here