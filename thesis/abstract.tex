\begin{abstract}
  Spiking Neural Networks (SNNs) represent an emerging paradigm in machine learning, inspired by biological neurons, with potential for greater energy efficiency compared to traditional Artificial Neural Networks (ANNs). Unlike ANNs, which use continuous floating-point outputs, SNNs produce discrete (binary) spikes when a neuronâ€™s membrane voltage exceeds a threshold.

  In this thesis we propose a novel firing model where neurons fire at multiple spiking levels (multi-bit spike trains) when their membrane potentials reach corresponding thresholds. We show that this model enables up to 51\% faster convergence and up to 2\% better validation accuracy on various tasks compared to the traditional 1-bit spike train model, while maintaining the energy efficiency of SNNs. We also present an energy consumption model in a real-world workflow and show the multi-bit spike train model can be up to 60\% more energy efficient than the 1-bit spike train model by reducing the number of time steps.
\end{abstract}
